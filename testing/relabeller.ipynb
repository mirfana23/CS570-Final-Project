{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle relative importing\n",
    "import os\n",
    "import sys\n",
    "module_paths = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "]\n",
    "for module_path in module_paths:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import necessary files\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "# dataloader\n",
    "from src.dataloaders.dataloader import ImgNetDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file of our validation set\n",
    "# relabel_json = json.load(open('/root/CS570-Final-Project/datasets/imgnet1k_original/val_relabelled.json', 'r')) # our relabeled validation set\n",
    "relabel_json = json.load(open('/root/CS570-Final-Project/datasets/imgnet1k_original/val.json', 'r')) # original validation set\n",
    "# json file of ReaL validation set\n",
    "real_json = json.load(open('/root/CS570-Final-Project/datasets/imgnet1k_ReaL/val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the labels per sample align\n",
    "labels_dict_relabel = {}\n",
    "for sample in relabel_json['data']:\n",
    "    labels_dict_relabel[sample['img_path']] = sample['labels']\n",
    "labels_dict_real = {}\n",
    "for sample in real_json['data']:\n",
    "    labels_dict_real[sample['img_path']] = sample['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes:\n",
    "- we skip the samples in ReaL that has not beed assigned any labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8234973638735616\n"
     ]
    }
   ],
   "source": [
    "avg_jaccard_sim = 0\n",
    "cnt = 0\n",
    "for key in labels_dict_relabel.keys():\n",
    "    labels_relabel = set(labels_dict_relabel[key])\n",
    "    labels_real = set(labels_dict_real[key])\n",
    "    if len(labels_real) == 0:\n",
    "        continue\n",
    "    jaccard_sim = len(labels_relabel.intersection(labels_real)) / len(labels_relabel.union(labels_real))\n",
    "    avg_jaccard_sim += jaccard_sim\n",
    "    cnt += 1\n",
    "avg_jaccard_sim /= cnt\n",
    "print(avg_jaccard_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average Precision / Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9002284518649786, 0.8234973638735616)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prec, avg_recall = 0, 0\n",
    "cnt = 0\n",
    "for key in labels_dict_relabel.keys():\n",
    "    labels_relabel = set(labels_dict_relabel[key])\n",
    "    labels_real = set(labels_dict_real[key])\n",
    "    if len(labels_real) == 0:\n",
    "        continue\n",
    "    precision = len(labels_relabel.intersection(labels_real)) / len(labels_relabel)\n",
    "    recall = len(labels_relabel.intersection(labels_real)) / len(labels_real)\n",
    "    avg_prec += precision\n",
    "    avg_recall += recall\n",
    "    cnt += 1\n",
    "avg_prec /= cnt\n",
    "avg_recall /= cnt\n",
    "avg_prec, avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846062941691404"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1 = 0\n",
    "cnt = 0\n",
    "for key in labels_dict_relabel.keys():\n",
    "    labels_relabel = set(labels_dict_relabel[key])\n",
    "    labels_real = set(labels_dict_real[key])\n",
    "    if len(labels_real) == 0:\n",
    "        continue\n",
    "    precision = len(labels_relabel.intersection(labels_real)) / len(labels_relabel)\n",
    "    recall = len(labels_relabel.intersection(labels_real)) / len(labels_real)\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    avg_f1 += f1\n",
    "    cnt += 1\n",
    "avg_f1 /= cnt\n",
    "avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file of our validation set\n",
    "relabel_json = json.load(open('/root/CS570-Final-Project/datasets/imgnet1k_original/val.json', 'r')) # original validation set\n",
    "# json file of ReaL validation set\n",
    "real_json = json.load(open('/root/CS570-Final-Project/datasets/imgnet1k_ReaL/val.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42164"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict_relabel = {}\n",
    "for sample in relabel_json['data']:\n",
    "    labels_dict_relabel[sample['img_path']] = sample['labels']\n",
    "labels_dict_real = {}\n",
    "for sample in real_json['data']:\n",
    "    labels_dict_real[sample['img_path']] = sample['labels']\n",
    "\n",
    "# check whether validation set is a subset of ReaL validation set\n",
    "cnt = 0\n",
    "for key in labels_dict_relabel.keys():\n",
    "    labels_relabel = set(labels_dict_relabel[key])\n",
    "    labels_real = set(labels_dict_real[key])\n",
    "    if labels_relabel.issubset(labels_real):\n",
    "        cnt += 1\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
